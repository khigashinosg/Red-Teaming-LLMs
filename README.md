# Red Teaming LLMs

## Description
Red Teaming LLMs is a project designed to assess the ethical protection measures of large language models (LLMs). Using OpenAI's API, it tests LLM responses to ethically challenging prompts, categorized as subtle and direct. This project aims to provide insights into the ethical and responsible use of AI in generating text.

## Getting started

This project uses Python scripts and Jupyter Notebooks to format prompts, collect responses, and analyze data from LLMs. Follow the steps below to set up and run the project.

### Add your files

- Clone the repository and navigate to the project directory:
git clone [repository URL]
cd Red Teaming LLMs

- Install the required dependencies:
pip install -r requirements.txt


### Installation and Usage

- Ensure you have Python 3.10 and access to OpenAI's API.
- Set your OpenAI API key in the code or as an environment variable.
- Run the Python scripts to format prompts and collect responses from LLMs:
python script_name.py


- Analyze the responses and visualize the data using the `model_evaluation.ipynb` Jupyter Notebook.

### Contributing

Contributions to the project are welcome. Please refer to the contributing guidelines for more information on how to participate.

### Project status

The project is in active development. Future updates will focus on refining the analysis and expanding the range of prompts and models tested.

---

**Please replace `[repository URL]` with the actual URL of your GitHub repository.** This README template is formatted with Markdown, providing clear headings and instructions for users of the "Red Teaming LLMs" project.
